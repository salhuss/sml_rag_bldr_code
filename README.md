# ğŸ“„ RAG Demo with Streamlit, OpenAI, and FAISS

This project is a **Retrieval-Augmented Generation (RAG) demo** that lets users upload their own documents (PDF, DOCX, TXT, etc.), automatically process them into embeddings, and interact with them through a chat-like interface powered by the **OpenAI SDK**.  

It uses:
- **Streamlit** for the user interface  
- **LangChain-style chunking** for text splitting  
- **OpenAI Embeddings API** to embed text chunks  
- **FAISS** as the vector database for fast similarity search  
- **OpenAI GPT models** (e.g., GPT-4, GPT-3.5) for grounded responses  

---

## ğŸš€ Features

- Upload documents (PDF, TXT, DOCX) directly in the browser  
- Automatic text extraction and chunking (configurable size & overlap)  
- Embedding with OpenAI (`text-embedding-ada-002`)  
- FAISS vector store for similarity search  
- Ask natural language questions about your uploaded documents  
- Streamlit chat-style interface  

---

## ğŸ—‚ Project Structure

streamlit-rag-demo/
â”‚
â”œâ”€â”€ app.py                # Main Streamlit app
â”œâ”€â”€ utils.py              # Helper functions (chunking, embedding, indexing)
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ README.md             # This file
â””â”€â”€ .env                  # API keys


---

## âš™ï¸ Setup

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/streamlit-rag-demo.git
cd streamlit-rag-demo

pip install -r requirements.txt

Key packages:
	â€¢	streamlit
	â€¢	faiss-cpu
	â€¢	openai
	â€¢	pypdf (for PDF parsing)
	â€¢	python-docx (for DOCX parsing)
	â€¢	tiktoken
	â€¢	langchain (optional, for chunking utilities)

3. Configure API Key

Create a .env file in the project root:

OPENAI_API_KEY=your_openai_api_key_here

ğŸ›  Usage

Run the Streamlit app:
streamlit run app.py

Then open http://localhost:8501 in your browser.

â¸»

ğŸ® How to Use
	1.	Upload a Document
Drag and drop a .pdf, .docx, or .txt file.
	2.	Processing
	â€¢	The document is split into smaller text chunks (default: 500 tokens with 100 overlap).
	â€¢	Each chunk is embedded with OpenAI and stored in FAISS.
	3.	Ask Questions
	â€¢	Enter a natural language question in the chat box.
	â€¢	The app retrieves the most relevant chunks and passes them along with your query to the LLM.
	â€¢	The response is shown in the chat interface with sources (if enabled).

â¸»

ğŸ’» Example Flow
	1.	Upload contract.pdf.
	2.	Ask:
â€œWhat are the termination conditions in this agreement?â€
	3.	The system:
	â€¢	Retrieves top-3 relevant chunks from FAISS
	â€¢	Constructs a prompt: â€œBased on the following text from the contract, answer the questionâ€¦â€
	â€¢	Generates a grounded answer

Output:
Termination conditions include: either party may terminate with 30 days notice,
breach of confidentiality results in immediate termination, etc.

ğŸ§© How It Works (Pipeline)
	1.	Upload â†’ User uploads document
	2.	Extract â†’ Convert PDF/DOCX/TXT into raw text
	3.	Chunk â†’ Split text into manageable segments
	4.	Embed â†’ Encode chunks using OpenAI embeddings
	5.	Index â†’ Store vectors in FAISS for similarity search
	6.	Query â†’ Retrieve top-k matches and feed them into GPT model
	7.	Generate â†’ Return grounded answer to user


â¸»

ğŸ“ˆ Roadmap
	â€¢	âœ… Support PDF, TXT, DOCX uploads
	â€¢	âœ… Local FAISS storage
	â€¢	ğŸ”„ Allow multiple documents at once
	â€¢	âš¡ Option to persist FAISS indexes across sessions
	â€¢	ğŸŒ Add Pinecone/Weaviate support for cloud vector DBs
	â€¢	ğŸ–¥ Deploy on Streamlit Cloud

ğŸ§ª Testing

To run quick tests on utilities:

pytest

ğŸ¤ Contributing

Pull requests welcome. Suggestions for new features (like semantic highlighting or multiple doc upload) are encouraged.
